\RequirePackage{xr-hyper}

% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamonline250211}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}
\usepackage{lipsum}


% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={An Example Article},
  pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

% \externaldocument[][nocite]{ex_supplement}[]
\externaldocument[][nocite]{ex_supplement_lemmas}[]

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Domain adaptation seeks to leverage the abundant label information in a source domain to improve classification performance in a target domain with limited labels. While the field has seen extensive methodological development, its theoretical foundations remain relatively underexplored. Most existing theoretical analyses focus on simplified settings where the source and target domains share the same input space and relate target-domain performance to measures of domain discrepancy. Although insightful, these analyses may not fully capture the behavior of modern approaches that align domains into a shared space via feature transformations. In this paper, we present a comprehensive theoretical study of domain adaptation algorithms based on \textit{domain alignment}. We consider the joint learning of domain-aligning feature transformations and a shared classifier in a semi-supervised setting. We first derive generalization bounds in a broad setting, in terms of covering numbers of the relevant function classes. We then extend our analysis to characterize the sample complexity of domain-adaptive neural networks employing maximum mean discrepancy (MMD) or adversarial objectives. Our results rely on a rigorous analysis of the covering numbers of these architectures. We show that, for both MMD-based and adversarial models, the sample complexity admits an upper bound that scales quadratically with network depth and width. Furthermore, our analysis suggests that in semi-supervised settings, robustness to limited labeled target data can be achieved by scaling the target loss proportionally to the square root of the number of labeled target samples. Experimental evaluation in both shallow and deep settings lends support to our theoretical findings.
\end{abstract}

% REQUIRED
\begin{keywords}
Domain adaptation, generalization bounds, domain-adaptive neural networks, maximum mean discrepancy, adversarial domain adaptation, sample complexity
\end{keywords}

% REQUIRED
\begin{MSCcodes}
68Q32, 68T05, 68T07
\end{MSCcodes}

%%% INTRODUCTION
\section{Introduction}
\label{sec:intro}

% DA: Purpose, applications, brief history
Domain adaptation is a subfield of machine learning that aims to improve model performance in a target domain by leveraging the greater availability of labeled samples in a source domain. The main challenge in domain adaptation is to address the discrepancy between the source and target distributions, which can take various forms such as covariate shift \cite{KouwL21}, label shift \cite{Azizzadenesheli19, Combes0WG20}, as well as more challenging heterogeneous settings  with source and target samples originating from different data spaces \cite{SinghalWRK23}. 
Early work in domain adaptation explored instance reweighting methods for covariate shift \cite{HuangSGBS06, SunCPY11}, feature augmentation approaches \cite{DaumeIII07, DaumeKS10, DuanXT12}, and techniques for learning feature projections or transformations \cite{BaktashmotlaghHLS13, PanTKY11, YaoPNLM15}. More recently, in line with broader advances in data science, domain adaptation research over the last decade has largely shifted towards deep learning-based techniques \cite{SinghalWRK23, WangD18}.  Metrics such as maximum mean discrepancy (MMD)   \cite{LongCWJ15, TzengHZSD14, GhifaryKZ14} lead to efficient solutions for aligning source and target domains across various applications \cite{ZengSRGFZ25, WangYXWZW23, DXiaLZSL25, YangZLLLC25}. Adversarial architectures \cite{GaninUAGLLML16, TzengHSD17, TangJ20, ZonooziS23}  and reconstruction-based approaches using encoder-decoder structures \cite{GhifaryKZBL16, BousmalisTSKE16, ZonooziSD25} are also commonly employed. 

Despite the variety of models and the diversity of solutions, the basic paradigm in domain adaptation - whether using shallow methods or neural networks- often boils down to first aligning the source and target domains by mapping them to a common space through feature transformations, followed by learning a hypothesis function, typically a classifier, in that shared domain. The alignment of the source and target distributions is achieved by minimizing a suitably defined \textit{distribution distance} (also referred to as \textit{domain discrepancy} or \textit{distribution divergence}), with common choices including MMD \cite{LongCWJ15}, covariance-based metrics \cite{SunS16}, and the Wasserstein distance \cite{CourtyFTR17, DamodaranKFTC18, HamriBF25}. Although domain adaptation algorithms have been successfully applied across a wide range of fields  including computer vision, time-series analysis, and natural language processing  \cite{SinghalWRK23, ZonooziS23}, surprisingly, the literature still lacks a thorough theoretical characterization of their performance. In particular, there is a notable gap in understanding the behavior of \textit{domain alignment algorithms}, which we define as methods that explicitly map source and target domains to a common representation through feature transformations. In this paper, we focus on this important class of algorithms, and aim to provide a rigorous theoretical analysis of their performance.




Most existing theoretical analyses focus on understanding how the discrepancy between source and target domains affects the target-domain performance of classifiers trained to perform well on the source domain \cite{RedkoMHS20, BenDavidBCP06, MansourMR09, ZhangLLJ19, DhouibRL20, WangM24}. While these studies provide useful insight into how models trained with abundant source labels generalize to a target domain with limited or no labeled data, they inherently assume that source and target data reside in the same space. Consequently, their results do not straightforwardly extend to the prevalent framework where source and target domains are aligned through feature transformations or mappings -whether shallow or deep- prior to classification. Only a few studies have investigated the performance of domain alignment algorithms \cite{ZhouTPT19, FangLLZ23, WangS15}; however, these works rather focus on specific transformation types, such as linear mappings \cite{ZhouTPT19} or location and scale changes \cite{WangS15}. Some literature has investigated the performance and sample complexity of transfer learning via deep learning approaches \cite{TomerWH16, McNamaraB17, JiaoLLY24}. However, domain adaptation and transfer learning remain distinct problems:  transfer learning deals with  differing source and target tasks, unlike domain adaptation. Notably, the characterization of the sample complexity of domain-adaptive neural networks remains an important yet largely unexplored subject in current learning theory. It is well established that the amount of data required to successfully train a neural network increases with the size of the network to prevent overfitting, and many studies have addressed this issue in classical single-domain settings \cite{AnthonyB02, NeyshaburTS15, WeiM19, VardiSS22, DanielyG24}.  To the best of our knowledge, however, the scaling of labeled and unlabeled source and target sample requirements with respect to the width and depth of domain-adaptive networks has not been extensively studied yet. 


In this work, we aim to fill this gap by providing a comprehensive theoretical analysis of domain adaptation in the widely used setting where the source and target domains are mapped to a common space through feature transformations, and a hypothesis is learnt in that shared space after alignment.  We consider a semi-supervised setting where labels are largely available for the source samples but limited (or unavailable) for the target samples. The structure of the paper along with our main contributions are summarized below:

% 3 line kazanmak için: 
\begin{itemize}[labelsep=0.5em, wide=0.5em]
% \begin{itemize}
\item In \cref{sec:gen_bounds}, we study a general setting that involves learning a source feature transformation $\fs \in \Fs$, a target feature transformation $\ft \in \Ft$ and a hypothesis $\h \in \Hs$ in the common domain. The learning objective minimizes a loss function composed of a weighted (convex) combination of the source and target classification losses, along with a distribution distance term that measures the discrepancy between the aligned domains. At this stage, our analysis remains general and does not assume any specific structure for the learning algorithm. In \cref{ssec:gen_bnd_arb_dist} (\cref{thm:gen_defect_target}), we present a probabilistic bound on the expected target loss in terms of the empirical weighted loss and the expected distribution discrepancy.  

\item  In \cref{ssec_gen_bnd_mmd} we develop these results for the setting where the distribution distance is selected as the popular maximum mean discrepancy (MMD) metric. In \cref{thm:main_result_mmd}, we show that  the expected target loss can be effectively bounded in terms of the empirical classification and distribution losses alone. This bound holds provided that the number of labeled source samples $\Ms$  scales logarithmically with the covering number of the composite hypothesis class $\Hs \circ \Fs$, while the total number of source and target samples, $\Ns$ and $\Nt$, must scale logarithmically with the covering numbers of the feature transformation classes $\Fs$ and $\Ft$. 

\item In \cref{ssec_samp_comp_mmd_net,ssec_adv_da_net} we extend our analysis to domain-adaptive deep learning algorithms and, in particular, investigate their sample complexity. We consider two pioneering approaches that have inspired a large body of follow-up work: MMD-based domain adaptation networks \cite{LongCWJ15, TzengHZSD14, GhifaryKZ14} and adversarial domain adaptation networks \cite{GaninUAGLLML16, TzengHSD17, TangJ20}. Our results in \cref{thm_main_result_da_mmd,thm_main_result_dann} show that, in both MMD-based and adversarial domain adaptation settings, the sample complexities for the number of labeled source samples $\Ms$ and the total number of source and target samples, $\Ns$ and  $\Nt$, scale quadratically with the width $\dcom$ and the depth $\numL$ of the network. Our results also offer insight into the optimal choice for the weight $\alpha$ of the target classification loss, indicating it should decrease at rate $\alpha=O(\sqrt{\Mt})$ to effectively handle the scarcity of labeled target samples.  Our proof technique extends \cref{thm:main_result_mmd} by thoroughly analyzing the covering numbers of the relevant function classes.  To the best of our knowledge, these are the first results to provide a comprehensive characterization of the sample complexity of domain-adaptive neural networks.
\end{itemize}


We defer a detailed discussion of closely related literature to \cref{sec_rel_work}, where we also compare and contrast our results with previous findings.  \cref{sec:exp_results} presents some simulation results for the experimental validation of our findings, and \cref{sec:conclusion} concludes the paper. A preliminary version of our study was presented in \cite{Vural18}, which laid the groundwork for the results in \cref{ssec:gen_bnd_arb_dist}.  


\clearpage

\section{General performance bounds for domain alignment}
\label{sec:gen_bounds}

\subsection{Problem formulation}
\label{ssec:problem_form}

Let $\Xs$ and $\Xt$ denote two compact metric spaces representing respectively a source domain and a target domain, and let $\Y \subset \R^m$ be a label set. Let $\mus$ be a source Borel probability measure and $\mut$ be a target Borel probability measure respectively on the sets $\Zs = \Xs \times \Y$ and $\Zt = \Xt \times \Y$. We consider the family of learning algorithms that aim to learn two mappings (transformations) $\fs: \Xs \rightarrow \X$ and $\ft: \Xt \rightarrow \X$ from the source and target domains to a common set $\X$ together with a hypothesis function $h: \X \rightarrow \Y $ estimating class labels on $\X$. The expected losses of the transformations $\fs$, $\ft$, and the hypothesis $h$ at the source and target are respectively given by
\begin{align*}
\Ls( \fs, \h) &= \int_{\Zs} \loss( \h \circ \fs(\xs), \ys ) \, d \mus&
\Lt( \ft, \h) &= \int_{\Zt} \loss( \h \circ \ft(\xt), \yt ) \, d \mut 
\end{align*}
where $\loss: \Y \times \Y \rightarrow [0, \infty)$ is a loss function.  Assuming that $\fs$ and $\ft$ are measurable mappings, the probability measures $\mus$ and $\mut$ on the source and target domains induce corresponding probability measures $\nus$ and $\nut$ on the domain $\X$. Let $\D$ be a function such that $\D (\fs, \ft)$ represents the distance between the measures $\nus$ and $\nut$ on $\X$ induced via the mappings $\fs$ and $\ft$ with respect to some distribution discrepancy criterion.   

Let $\{ \xis \}_{i=1}^{\Ns}$ be a set of source samples and $\{ \xjt \}_{j=1}^{\Nt}$ be a set of target samples drawn independently from the probability measures $\mus$ and $\mut$, where $\{ \xis \}_{i=1}^{\Ms}$ are the $\Ms$ labeled samples in the source with labels $\{ \yis \}_{i=1}^{\Ms}$, and $\{ \xjt \}_{j=1}^{\Mt}$ are the $\Mt$ labeled samples in the target with labels $\{ \yjt \}_{j=1}^{\Mt}$. We consider learning algorithms that minimize a convex combination of the source and target empirical losses, while minimizing the distance between the transformed source and target samples in the domain $\X$ as
%
\begin{equation}
\label{eq:obj_learning}
\begin{split}
\min_  {\fs \in \Fs,  \  \ft \in \Ft, \ \h \in  \Hs}
(1-\alpha) \hLs (\fs, \h) + \alpha \hLt (\ft, \h) 
+ \beta \hD (\fs, \ft).
\end{split}
\end{equation}
%
Here $\Fs$ and $\Ft$ are function classes consisting of a family of transformations, respectively from the source and target domains $\Xs$ and $\Xt$ to $\X$; $\Hs$ is a hypothesis class consisting of hypotheses; $\alpha$ is a weight parameter with $0 \leq \alpha \leq 1$; $\hLs (\fs, \h)$ and $\hLt (\ft, \h) $ are the empirical source and target losses given by
%
\begin{align}
\label{eq_emp_cl_loss}
\hLs( \fs, \h) &= \frac{1}{\Ms} \sum_{i=1}^{\Ms}  \loss( \h \circ \fs(\xis), \yis )&
\hLt( \ft, \h) &=  \frac{1}{\Mt} \sum_{j=1}^{\Mt} \loss( \h \circ \ft(\xjt),  \yjt  ) 
\end{align}
%
and the distance $\hD$ is an estimate of the distribution distance $\D (\fs, \ft)$ computed with all (labeled and unlabeled) samples $\{ \xis \}_{i=1}^{\Ns}$ and $\{ \xjt \}_{j=1}^{\Nt}$. As discussed in \cref{sec:intro}, the distribution distance $\D(\fs, \ft)$ has been chosen in different ways in previous works such as the MMD or Wasserstein distance along with the corresponding estimates $\hD(\fs, \ft)$ that lead to practical learning algorithms. In \cref{ssec:gen_bnd_arb_dist}, we provide generalization bounds for  learning algorithms with an arbitrary distribution distance function. Then in \cref{ssec_gen_bnd_mmd}, we focus on the kernel mean matching (KMM) methods in particular, and propose bounds for algorithms using a KMM-based distribution distance.


\subsection{Generalization bounds for arbitrary distribution distances}
\label{ssec:gen_bnd_arb_dist}

% \begin{figure}[t]
% \begin{center}
%      \subfloat[a)]
%        {\label{fig:illus_domain_relation_a}\includegraphics[width=0.4\textwidth]{figures/illus_domain_relation_a.pdf}}
%        \hspace{1cm}
%      \subfloat[b)]
%        {\label{fig:illus_domain_relation_b}\includegraphics[width=0.4\textwidth]{figures/illus_domain_relation_b.pdf}}
%  \end{center}
%  \caption{Illustration of \cref{assum_existence_LLs}. Red and blue colors represent two different classes in the source and target domains $\Xs$ and $\Xt$. In (a), the two domains are well-aligned by the learnt transformations; therefore, the source and target losses are similar. In (b), the learnt transformations do not align the domains well; therefore, the difference between the source and target losses can be high.}
%  \label{fig:illus_domain_relation}
% \end{figure}


\begin{figure}[tbhp]
\centering
\subfloat[]{\label{fig:illus_domain_relation_a}\includegraphics[width=0.4\textwidth]{figures/illus_domain_relation_a.pdf}}
\hspace{0.1\textwidth}
\subfloat[]{\label{fig:illus_domain_relation_b}\includegraphics[width=0.4\textwidth]{figures/illus_domain_relation_b.pdf}}
\caption{Illustration of \cref{assum_existence_LLs}. Red and blue colors represent two different classes in the source and target domains $\Xs$ and $\Xt$. In (a), the two domains are well-aligned by the learnt transformations; therefore, the source and target losses are similar. In (b), the learnt transformations do not align the domains well; therefore, the difference between the source and target losses can be high.}
\label{fig:illus_domain_relation}
\end{figure}


In order to analyze the performance of algorithms that aim to solve  \eqref{eq:obj_learning}, we first assume that the expected loss has a bounded rate of variation with respect to the chosen distribution distance: 

\begin{assumption}
\label{assum_existence_LLs}
There exists a constant $\LLs>0$ such that, for any transformations $\fs \in \Fs$, $\ft \in \Ft$ and any hypothesis $\h \in \Hs$, we have
%
\begin{equation}
\label{eq:Lipsc_reg_loss}
| \Ls(\fs, \h) - \Lt(\ft, \h) | \leq \LLs \ \D(\fs, \ft). 
\end{equation}
\end{assumption}

%
\cref{assum_existence_LLs} imposes the presence of a relation between the source and target distributions: The source and target distributions must be ``related'' in such a way that, when their distance is reduced in the common domain after going through the transformations in $\Fs$, $\Ft$,  their resulting losses should not differ too much compared to the distribution distance $D(\fs, \ft)$. This assumption is illustrated in \cref{fig:illus_domain_relation}. The figure depicts a simple setting where the source and target domains are aligned by geometric transformations $\fs$, $\ft$, which are respectively in the geometric transformation families $\Fs$ and $\Ft$. The hypothesis family $\Hs$ consists of linear classifiers $\h$.  In \cref{fig:illus_domain_relation_a}, the learnt transformations $\fs$ and $\ft$  suitably align the two domains, so that the distribution distance $\D(\fs, \ft)$ is small. Consequently, a hypothesis $h_1$ that yields a small loss $ \Ls(\fs, \h_1) $ in the source domain also yields a small loss $\Lt(\ft, \h_1)$ in the target domain; and a hypothesis $h_2$ that yields a large loss $ \Ls(\fs, \h_2) $ in the source domain also yields a large loss $\Lt(\ft, \h_2)$ in the target domain. Meanwhile, in \cref{fig:illus_domain_relation_b} the learnt transformations $\fs$ and $\ft$ do not align the two domains well. In this case, the distribution distance $\D(\fs, \ft)$ is large, which allows the loss difference $| \Ls(\fs, \h) - \Lt(\ft, \h) |$ also to be large by \cref{assum_existence_LLs}. Indeed, one may find a hypothesis $h$ that yields a small loss  $ \Ls(\fs, \h) $ in the source domain, but a large loss  $\Lt(\ft, \h)$ in the target domain. Since the loss difference $| \Ls(\fs, \h) - \Lt(\ft, \h) |$ can be bounded in terms of the distribution distance $\D(\fs, \ft)$, the transformation families  $\Fs, \Ft$, and the hypothesis family $\Hs$ considered in this example satisfy \cref{assum_existence_LLs}. In brief, the assumption dictates that there should be a sufficiently strong relation between the source and target domains, the function classes $\Fs$ and $\Ft$ must be chosen suitably to respect this relation, and the hypothesis family $\Hs$ must also be compatible with the problem. In the following, we first bound the expected target loss in terms of the expected weighted loss and the distribution distance.


We use the above relation to bound the expected target loss in terms of the empirical losses given by the learning algorithm. We characterize the complexity of the transformation and hypothesis classes in terms of their covering numbers, defined as follows \cite{CuckerS02}:%

\begin{definition}
Let $\F$ be a compact metric space with metric $\dmetric $, and let $B_\epsilon(f)$ denote an open ball of radius $\epsilon$ around $f \in \F$. Then the covering number $\N(\F, \epsilon, \dmetric)$ of $\F$ is defined as
%
\[
\N(\F, \epsilon, \dmetric) \triangleq \min \{ k: \exists f_1, \dots f_k \in \F, \ \F \subset \cup_{i=1}^k B_\epsilon(f_i) \}.
\]
\end{definition}
%

In order to study the discrepancy between the expected and the empirical losses, we next make the following assumptions. 
%
\begin{assumption}
\label{assum_HF_comp_Ll_Al}
The composite function classes $\Hs \circ \Fs \triangleq \{ \gs =  \h \circ \fs : \h \in \Hs, \fs \in \Fs \}$ and $\Hs \circ \Ft \triangleq \{ \gt = \h \circ \ft : \h \in \Hs, \ft \in \Ft \}$ are compact metric spaces with respect to the metrics
%
\begin{align}
\label{eq_defn_ds_dt}
\ds(\gs_1, \gs_2) & \triangleq \sup_{\xs \in \Xs} \| \gs_1(\xs) - \gs_2(\xs)  \|&
\dt(\gt_1, \gt_2) & \triangleq \sup_{\xt \in \Xt} \| \gt_1(\xt) - \gt_2(\xt)  \|
\end{align}
%
where $\| \cdot \|$ denotes the $l_2$-norm in $\R^m$.
Also, the loss function $\loss$ is bounded by $\bls$ and Lipschitz continuous with respect to the first argument with constant $\Lls$, such that
%
\begin{equation*}
\begin{split}
\loss(\y_1, \y_2) &\leq \bls, \  \forall \y_1, \y_2 \in \Y  \\
| \loss(\y_1, \y) -  \loss(\y_2, \y) | &\leq \Lls \| \y_1 - \y_2 \|,\  \forall \y_1, \y_2, \y \in \Y  .
\end{split}
\end{equation*}
\end{assumption}

To establish generalization guarantees, we first relate the expected target loss to a weighted combination of source and target losses. By weighting the losses with parameter $\alpha$, we obtain an upper bound on the expected target loss in terms of the expected weighted loss and the distribution discrepancy (\cref{lem:dev_exptar_expweigh}). Next, to connect this to empirical quantities, we bound the deviation between expected and empirical weighted losses using covering number arguments combined with Hoeffding's inequality (\cref{lem:weight_loss_gen}). Combining these two results yields the following generalization bound for the expected target loss.
%
\begin{theorem}
\label{thm:gen_defect_target}
Let Assumptions \ref{assum_existence_LLs}, \ref{assum_HF_comp_Ll_Al} hold. Then for any transformations $\fs \in \Fs$, $\ft \in \Ft$ and hypothesis $\h \in \Hs$, with probability at least
%
\begin{equation}
\label{eq_prob_expr_thm1}
\begin{split}
1 &- 2 \N( \Hs \circ \Ft, \frac{\epsilon}{8 \alpha \Lls}, \dt) e^{-\frac{\Mt \epsilon^2}{8 \alpha^2 \bls^2}} 
-2 \N( \Hs \circ \Fs, \frac{\epsilon}{8 (1-\alpha) \Lls}, \ds) e^{-\frac{\Ms \epsilon^2}{8 (1-\alpha)^2 \bls^2}}
\end{split}
\end{equation}
%
the expected target loss is bounded as
%
\[
\Lt(\ft, \h) \leq   \hLw(\fs, \ft, \h)+ (1-\alpha) \LLs  \D(\fs, \ft) + \epsilon.
\]
%
\end{theorem}
%
The main result in \cref{thm:gen_defect_target} states the following: For any algorithm that computes transformations $\fs$, $\ft$, and a hypothesis $\h$ by attempting to solve a problem such as in \cref{eq:obj_learning}, the actual expected loss obtained at the target  by applying the learnt transformation $\ft$ and hypothesis $\h$ to target test samples cannot differ from the empirical weighted loss  $\hLw(\fs, \ft, \h)$ obtained over training samples by more than $\epsilon$ plus an error term involving the distance $ \D(\fs, \ft) $. This statement holds with probability approaching 1 at an exponential rate with the increase in number of labeled samples $\Ms$. Note that in the very typical case where $\Mt$ is limited, the target term in the probability expression \cref{eq_prob_expr_thm1} can be controlled by suitably scaling down the weight parameter $\alpha$ proportionally to $O(\sqrt{\Mt})$. \footnote{ An important question is how much the learning algorithm is expected to reduce the distribution distance $\D(\fs, \ft)$. This depends on the chosen distance; nevertheless, in many practical learning problems, the number of unlabeled samples $\Ns, \Nt$ is much larger than the number of labeled samples $\Ms, \Mt$. If we assume that $N = \min( \Ns, \Nt) $ is sufficiently large, then we may expect the deviation between the expected and empirical distribution distances to decay such that
%
$%
P(  | \D(\fs, \ft) - \hD(\fs, \ft) |  \geq \epsilon ) 
&\leq (\N_{\Fs, \epsilon} + \N_{\Ft, \epsilon}) \ O \left( e^{-  N \epsilon^2 } \right)  \\
&\leq O \left( e^{-  \Mt \epsilon^2 } \right) 
+ O \left( e^{-  \Ms \epsilon^2 } \right) 
$%
%
for some appropriate complexity measures $\N_{\Fs, \epsilon}$ , $\N_{\Ft, \epsilon}$ for the transformation function classes. In this case, the result in \cref{thm:gen_defect_target} would imply that with  probability $1- O( e^{-  \Mt \epsilon^2 } ) - O( e^{-  \Ms \epsilon^2 } ) $, the expected target loss would be bounded in terms of the empirical losses and the empirical distribution distance as $\Lt(\ft, \h) &\leq   \hLw(\fs, \ft, \h) 
+ (1-\alpha) \LLs  \hD(\fs, \ft) + \epsilon + (1-\alpha) \LLs \epsilon$.
%
Our purpose in \Cref{ssec_gen_bnd_mmd} is to establish such a result for the particular setting where the distribution distance is chosen as the MMD.}

% Sonnet 4.5 önerisi (teorem sonundan itibaren): 
% bu bir düşünülsün değerlendirilsin. 
% The main result in \cref{thm:gen_defect_target} states the following: For any algorithm that computes transformations $\fs$, $\ft$, and a hypothesis $\h$ by attempting to solve a problem such as in \cref{eq:obj_learning}, the actual expected loss obtained at the target  by applying the learnt transformation $\ft$ and hypothesis $\h$ to target test samples cannot differ from the empirical weighted loss  $\hLw(\fs, \ft, \h)$ obtained over training samples by more than $\epsilon$ plus an error term involving the distance $ \D(\fs, \ft) $. This statement holds with probability approaching 1 at an exponential rate with the increase in number of labeled samples $\Ms$. Note that when $\Mt$ is limited, the target term in \cref{eq_prob_expr_thm1} can be controlled by scaling $\alpha$ proportionally to $O(\sqrt{\Mt})$. When unlabeled samples $\Ns, \Nt$ are abundant relative to labeled samples, concentration results for $\D(\fs, \ft)$ enable expressing the bound in terms of empirical quantities $\hLw$ and $\hD$—we establish this for MMD in \cref{ssec_gen_bnd_mmd}.


\subsection{Generalization bounds for maximum mean discrepancy measures}
\label{ssec_gen_bnd_mmd}

We now extend the results of \cref{ssec:gen_bnd_arb_dist} for a setting where the distribution discrepancy in the common domain of transformation is measured with respect to the maximum mean discrepancy (MMD) criterion. The MMD criterion is widely used in domain adaptation. In particular, a popular family of methods called kernel mean matching (KMM) algorithms aim to map the source and target data to a shared domain via a kernel function such that the distance between the source and target samples measured with respect to the MMD criterion is minimized.

KMM methods set the source and target mappings $\fs: \Xs \rightarrow \X$ and $\ft: \Xt \rightarrow \X$ as a kernel-induced feature map $\phi$. The source and target domains $\Xs = \Xt$ are often assumed to be the same and the transformations are set as $\fs=\ft=\phi$. The shared domain $\X$ is typically a Hilbert space with a kernel $\kr: \Xs \times \Xt \rightarrow \R$ satisfying $\kr(\xs, \xt) = \langle \phi(\xs), \phi(\xt) \rangle_{\X}$ with respect to the inner product $\langle \cdot, \cdot \rangle_{\X}$ in $\X$.

Given the source and target probability measures $\mus$, $\mut$ on the sets $\Zs = \Xs \times \Y$ and $\Zt = \Xt \times \Y$; and the probability measures $\nus$, $\nut$ these respectively induce over the domain $\X$; KMM algorithms characterize the distance between $\nus$ and $\nut$ via the MMD given by 
%
\begin{equation}
\label{eq:defn_D_MMD}
\D(\fs, \ft) = \|  E_{\xs}[\fs(\xs)] - E_{\xt}[ \ft(\xt) ] \|_{\X}
\end{equation}
%
where $\| \cdot \|_{\X}$ stands for the inner-product-induced norm in the Hilbert space $\X$. \footnote{For notational simplicity, we will drop the subscript $(\cdot)_{\X}$ when there is no ambiguity over the space in consideration.  The notation $ E_{\xs}[\cdot] $ and $ E_{\xt}[\cdot] $ indicates that the expectations are taken with respect to the probability measures $\mus$ and $\mut$ in the source and the target domains, respectively. We will simply write $E[\cdot]$ whenever the meaning is clear.}Given the source and target sample sets $\{ \xis \}_{i=1}^{\Ns} $ and $\{ \xjt \}_{j=1}^{\Nt}$, the empirical estimate of the MMD is given by
%
\begin{equation}
\label{eq:defn_hD_MMD}
\hD(\fs, \ft) = \left \| 
\frac{1}{\Ns} \sum_{i=1}^{\Ns} \fs(\xis) -  \frac{1}{\Nt} \sum_{j=1}^{\Nt} \ft(\xjt) 
\right \|.
\end{equation}
%


In order to study the performance of KMM algorithms,  we first derive a bound on the deviation between the actual distribution discrepancy $\D(\fs, \ft)$ and its empirical estimate $\hD(\fs, \ft)$.\footnote{Although most KMM methods assume $\Xs = \Xt$ and $\fs=\ft=\phi$, we do not make these assumptions. We only assume that the distribution discrepancy between $\nus$ and $\nut$ is taken as in \eqref{eq:defn_D_MMD} and the empirical estimate is computed as in \eqref{eq:defn_hD_MMD}.} We make the following assumption on the data distributions:



\begin{assumption}
\label{assum_fx_bdd_moments}
The random variables ${ \fs(\xis) }{i=1}^{\Ns}$ and ${ \ft(\xjt) }{j=1}^{\Nt}$ have bounded expected deviations from their respective means $E[\fs(\xs)]$ and $E[\ft(\xt)]$; that is, there exist constants $\vars$ and $\vart$ such that
%
\begin{align}
\label{eq:var_bnd_fs_ft}
E \left[  \| \fs(\xis) - E[\fs(\xs)]  \|^2 \right]  &\leq \vars &
E \left[  \| \ft(\xjt) - E[\ft(\xt)]  \|^2 \right]  &\leq \vart.
\end{align}
%
Also, for the higher order powers of the deviation, there exist constants $\Cs$ and $\Ct$ satisfying
%
\begin{align}
\label{eq:mom_bnd_fs_ft}
E \left[  \| \fs(\xis) - E[\fs(\xs)]  \|^k \right]  &\leq \frac{k!}{2} \vars \, \Cs^{k-2}&
E \left[  \| \ft(\xjt) - E[\ft(\xt)]  \|^k \right]  &\leq \frac{k!}{2} \vart \, \Ct^{k-2}.
\end{align}
\end{assumption}
%
The condition \eqref{eq:var_bnd_fs_ft} can be seen as a finite variance assumption for a distribution over a Hilbert space, and the condition \eqref{eq:mom_bnd_fs_ft} bounds the growth of the $k$-th central moment by a rate of $O(k! \, C^{k})$. These assumptions hold for many common data distributions in practice.





To analyze the MMD estimator, we first establish concentration of sample means in Hilbert spaces. Using a Bernstein-type inequality due to Yurinskii \cite{Yurinski76}, we show that deviations of empirical means from expectations decay exponentially in the sample size (\cref{lem:bernstein_src_trg}). Building on this, we derive uniform bounds on $| \D(\fs, \ft) - \hD (\fs, \ft)  |$ that hold simultaneously for all transformation pairs in $\Fs \times \Ft$ by constructing finite covers of these function classes and applying union bounds (\cref{lem:unif_bnd_D_hD}). This uniformity requires the following compactness assumption.

\begin{assumption}
\label{assum_Fs_Ft_compact}
The function classes $\Fs$ and $\Ft$ are compact metric spaces with respect to the metrics
\begin{align}
\label{eq_defn_dXs_dXt}
\dXs (\fs_1 , \fs_2)  &\triangleq \sup_{\xs \in \Xs}  \|  \fs_1(\xs) - \fs_2(\xs) \| &
\dXt (\ft_1 , \ft_2)  &\triangleq \sup_{\xt \in \Xt}  \|  \ft_1(\xt) - \ft_2(\xt) \|.
\end{align}
\end{assumption}

Having established concentration properties for the MMD estimator, we can now extend \cref{thm:gen_defect_target} by replacing the expected distribution discrepancy $\D(\fs, \ft)$ with its empirical estimate $\hD(\fs, \ft)$, yielding a fully empirical generalization bound. 


\begin{theorem}
\label{thm:main_result_mmd}

Consider a domain adaptation algorithm where the distribution discrepancy is taken as the MMD measure, and the loss function and data distributions satisfy \cref{assum_existence_LLs,assum_Fs_Ft_compact}. For $\epsilon >0$, let the number of source and target samples satisfy
$\Ns > \frac{16 \vars}{\epsilon^2}$ and  $\Nt > \frac{16 \vart}{\epsilon^2}$.
Then for any transformations $\fs \in \Fs$, $\ft \in \Ft$, and hypothesis $\h \in \Hs$, with probability at least
\begin{equation}
  \begin{split}
    1 &- 2 \N( \Hs \circ \Ft, \frac{\epsilon}{8 \alpha \Lls}, \dt) e^{-\frac{\Mt \epsilon^2}{8 \alpha^2 \bls^2}} 
    -2 \N( \Hs \circ \Fs, \frac{\epsilon}{8 (1-\alpha) \Lls}, \ds) e^{-\frac{\Ms \epsilon^2}{8 (1-\alpha)^2 \bls^2}} \\
    & - \N(\Fs, \frac{\epsilon}{8}, \dXs) e^{-\as(\Ns, \epsilon)}
    - \N(\Ft, \frac{\epsilon}{8}, \dXt) e^{-\at(\Nt, \epsilon)}
  \end{split}
\end{equation}
the expected target loss is upper bounded as
%
\begin{equation}
\Lt(\ft, \h) \leq   \hLw(\fs, \ft, \h)+ (1-\alpha) \LLs  \hD(\fs, \ft) + (1-\alpha) \LLs \epsilon + \epsilon.
\end{equation}

{The proof follows from \cref{thm:gen_defect_target} and \cref{lem:unif_bnd_D_hD} by the union bound.}
%
\end{theorem}


The result in \cref{thm:main_result_mmd} states that the target loss can be bounded in terms of the empirical weighted loss and the empirical distribution discrepancy, with probability approaching 1 at an exponential rate as the number of labeled and unlabeled samples increases. The dependence of this rate on the number of unlabeled samples follows from the relations $\as(\Ns,\epsilon)=O(\Ns \epsilon^2)$ and $\at(\Nt, \epsilon) = O(\Nt \epsilon^2)$. In particular, our result points to the following practical fact: If a domain adaptation algorithm efficiently minimizes the empirical weighted loss and the empirical distribution discrepancy, the true loss obtained in the target domain will also be small, provided that the number of samples is sufficiently high with respect to the complexity of the transformation and hypothesis classes, characterized by their covering numbers.


\clearpage

\section{Sample complexity of domain-adaptive neural networks}
\label{sec_samp_dan}

In this section, we build on the results in \Cref{sec:gen_bounds} and extend our analysis to examine the performance of domain-adaptive neural networks. In particular, we study the sample complexity of two common neural network types, namely, MMD-based and adversarial architectures, respectively in \cref{ssec_samp_comp_mmd_net,ssec_adv_da_net}.

\subsection{MMD-based domain adaptation networks}
\label{ssec_samp_comp_mmd_net}

We study the implications of \cref{thm:main_result_mmd} on deep domain adaptation networks that learn domain-invariant features based on the MMD distance measure. We consider the network model depicted in \cref{fig_illus_mmd_network}, a commonly adopted foundation for many MMD-based architectures. Defining $\hsz \triangleq \xs \in \R^\dz$ and $\htz \triangleq \xt \in \R^\dz$, the relation between the features of layers $\lay$ and $\lay-1$ is given by
%
\begin{align}
\label{eq_defn_hsl_htl}
\hsl&=\actl( \Wsl \hslm + \bsl)&
\htl&=\actl( \Wtl \htlm + \btl)
\end{align}
%
for $\lay=1, \dots, \numL$, where $\hsl, \htl \in \Rdl$ are $\dl$-dimensional source and target features in layer $l$; the parameters $\Wsl, \Wtl \in \R^{\dl \times \dlm}$ are source and target weight matrices; the parameters $\bsl, \btl \in \Rdl $ are source and target bias vectors; $\actl: \Rdl \rightarrow \Rdl$ is a nonlinear activation function; $\numL$ is the depth of the network; and $\dl$ is the width of the network at layer $\lay$. We assume that the parameters of the output layer $\numL$ are common between the source and the target domains, such that $\WsL=\WtL=\WL \in \R^{m \times \dLm}$ and $\bsL=\btL=\bL \in \R^{m}$, where $m=\dL$ is the number of classes.


\begin{figure}[t]
  \centering
  \centerline{\includegraphics[width=14.0cm]{figures/illus_mmd_network.pdf}}
  \caption{Illustration of MMD-based domain adaptation networks. Source and target samples first pass through a common network (convolutional and fully connected layers), then through domain-specific networks of $\numL-1$ fully connected layers, with the $\numL$-th layer being a shared classifier. The common network parameters are often adopted from pre-trained networks or fine-tuned using source samples \cite{LongCWJ15,TzengHZSD14,GhifaryKZ14}; hence we consider feature representations at its output as our domain samples.}
  \label{fig_illus_mmd_network}
\end{figure}


Let $\Thetasl=[\Wsl \ \bsl] \in \R^{\dl \times (\dlm+1)}$ and $\Thetatl =[\Wtl \ \btl] \in \R^{\dl \times (\dlm+1)}$ denote the matrices containing the network parameters of layer $\lay$. Let us also define the overall parameter structures 
$%
\Thetas&=(\boldsymbol{\Theta}^{s1}, \dots, \boldsymbol{\Theta}^{s \numL})$ and $\Thetat&=(\boldsymbol{\Theta}^{t1}, \dots, \boldsymbol{\Theta}^{t \numL}) 
$ 
containing the parameters of the entire source and target networks, respectively. We model the source and target domains to be compact sets and the network parameters to be bounded.
%
% bu assumption in-line olsaydı 3 line kazanırdık.
\begin{assumption}
\label{assum_Ax_Atheta}
The source and target domains are % given by
\begin{align}
\label{eq_bnd_xs_xt}
\Xs &= \{  \xs \in \R^\dz : \| \xs \| \leq \Binp \} &
\Xt &= \{  \xt \in \R^\dz : \| \xt \| \leq \Binp \}
\end{align}
for some bound $\Binp>0$. Also, the network parameters $\Thetasl$, $\Thetatl $ in each layer belong to a closed and bounded set in $\R^{\dl \times (\dlm+1)} $ such that 
%
\begin{equation}
\label{eq_bnd_Thetaij}
|\Thetasl_{ij}| , |\Thetatl_{ij}| \leq \Bnet
\end{equation}
%
for some magnitude bound parameter $\Bnet>0$, for $\lay=1, \dots, \numL$ and $i=1, \dots, \dl$; $j=1, \dots, \dlm+1$.
%
\end{assumption}


Clearly, the features $\hsl$, $\htl$ in all layers depend on both the input vectors $\xs$, $\xt$ and the network parameters $\Thetas $, $\Thetat$. In the following, with a slight abuse of notation we write $\hsl_{\Thetas}$ when we would like emphasize the dependence of $\hsl$ on the network parameters $\Thetas$, and we write $\hsl (\xs)$ when we would like to refer to the dependence of $\hsl$ on the input $\xs$. The notation is set similarly for the target domain variables. % burası da footnote'a alınabilir




MMD-based deep domain adaptation networks employ a feature mapping $\phil: \Rdl \rightarrow \Xl $ between the hidden layer feature vectors $\hsl, \htl$ and a Reproducing Kernel Hilbert Space (RKHS) $\Xl$ \cite{LongCWJ15, GrettonBRSS12}. The RKHS $\Xl$ of each layer $\lay$ has a symmetric, positive definite characteristic kernel $\krl: \Rdl \times \Rdl \rightarrow \R$ such that
%
$%
\krl(\hidl_1, \hidl_2 ) = \langle \phil(\hidl_1), \phil(\hidl_2) \rangle_{\Xl}
$ 
%
for any $\hidl_1, \hidl_2 \in \Rdl$, where $\langle \cdot, \cdot \rangle_{\Xl}$ denotes the inner product in the RKHS $\Xl$ \cite{GrettonBRSS12}. The feature mapping $\phil $ and the characteristic kernel $\krl$ are related as $\phil(\hidl)=\krl(\hidl,\cdot): \Rdl \rightarrow \R$ \cite{GrettonBRSS12}. The feature mapping $\phil $ has the property that  $\langle \phil(\hidl), \psi  \rangle_{\Xl} = \psi(\hidl)$  for any  $\psi \in \Xl$ and $\hidl \in \Rdl$.

In order to study this common framework within the setting of \cref{ssec_gen_bnd_mmd}, let us first define the functions $\fsl: \Xs \rightarrow \Xl$ and  $\ftl: \Xt \rightarrow \Xl$ as
\begin{align}
\label{eq_defn_fsl_ftl}
\fsl(\xs) &\triangleq \phil(\hsl(\xs)) \in \Xl &
\ftl(\xt) &\triangleq \phil(\htl(\xt))\in \Xl 
\end{align}
for $\lay=1, \dots \, , \numL-1$. Note that the direct sum 
%
$%%
\X=\bigoplus_{\lay=1}^{\numL-1} \Xl = \{ (f^1, f^2, \dots \, , f^{\numL-1}) : f^\lay \in \Xl, \, \lay=1, \dots, \numL-1 \}
$ 
 of the RKHSs $\X^1, \dots \, , \X^{\numL-1}$ is also a Hilbert space with inner product $\langle \cdot , \cdot \rangle_{\X}$ given by \cite{DunfordS88}
\begin{equation}
\label{eq_defn_inn_prod_X}
\langle (f^1,  \dots \, ,  f^{\numL-1}) ,  (g^1,  \dots \, , g^{\numL-1}) \rangle_{\X}
= \sum_{\lay=1}^{\numL-1} 
\langle f^\lay, g^\lay \rangle_{\Xl}.
\end{equation}
%

Let us use the notation $\fsl_{\Thetas}(\xs)$ and $\ftl_{\Thetat}(\xt)$ for the functions $\fsl(\xs)$ and $\ftl(\xt)$ defined in \eqref{eq_defn_fsl_ftl} whenever we would like to emphasize their dependence on the network parameters. We can now define the function spaces
%
\begin{equation}
\label{eq_Fs_Ft_defn_dl}
\begin{split}
\Fs &= \{ \fs: \Xs \rightarrow \X \ | \ 
\fs(\xs)=\big( \fsone_{\Thetas} (\xs), \dots \, , \fsLm_{\Thetas}(\xs)    \big) \in \X, 
\ |\Thetasl_{ij}|  \leq \Bnet, \forall i, j\}  \\
\Ft &= \{ \ft: \Xt \rightarrow \X \ | \ 
\ft(\xt)= \big( \ftone_{\Thetat} (\xt), \dots \, , \ftLm_{\Thetat}(\xt)    \big) \in \X, 
\ |\Thetatl_{ij}|  \leq \Bnet, \forall i, j \}
\end{split}
\end{equation}
%
which define the mapping from the source and target domains to the feature representations composed of all layers from $\lay=1$ up to $ \lay= \numL-1$. As these features are passed through layer $\lay=\numL$ for the final classification stage, we can regard the network outputs  $\hsL $, $\htL $ as the composition of the mappings $\fs$, $\ft$ with the hypothesis function $\h$, i.e.,
%
\begin{align}
\label{eq_gs_gt_defn}
\gs(\xs) &= (\h \circ \fs)(\xs) \triangleq \hsL(\xs)&
\gt(\xt) &= (\h \circ \ft)(\xt) \triangleq \htL(\xt).
\end{align}
%
Let us also define the corresponding function spaces
\begin{equation}
\label{eq_Gs_Gt_defn_dl}
\begin{split}
\Gs &= \Hs \circ \Fs = \{ \gs: \Xs \rightarrow \Y \ | \ 
\gs(\xs)= \hsL_{\Thetas}(\xs) \in \Y \subset \R^m, 
\ |\Thetasl_{ij}|  \leq \Bnet, \forall i, j\} \\
\Gt &= \Hs \circ \Ft = \{ \gt: \Xt \rightarrow \Y \ | \ 
\gt(\xt)= \htL_{\Thetat}(\xt) \in \Y \subset \R^m, 
\ |\Thetatl_{ij}|  \leq \Bnet, \forall i, j\} .
\end{split}
\end{equation}
%

In the following, we first assume the continuity of the kernels and the activations. 

\begin{assumption}
\label{assum_krl_actl_cont}
The kernels $\krl(\cdot , \cdot ) $ for layers $\lay=1, \dots, \numL-1$ and the activation functions $\actl (\cdot)$ for layers $\lay=1, \dots, \numL$ are continuous.
\end{assumption}



As stated in \cref{lem_fs_ft_measble} and proved in \cref{pf_lem_fs_ft_measble}, this assumption ensures that $ E[\fs(\xs)]$ and $ E[\ft(\xt)]$ are in $ \X $. % Hüseyin: Bu Lemma 5 yerine geçiyor. Lemma 5'in statement'ini de kaldırdık.

We next revisit the distribution discrepancy definition in \cref{ssec_gen_bnd_mmd} for MMD-based neural networks. Let us define the distribution discrepancy in layer $\lay$ as
%
$%
\D^\lay (\fsl, \ftl) \triangleq  \|  E_{\xs}[\fsl(\xs)] - E_{\xt}[ \ftl(\xt) ] \|_{\Xl}.
$ MMD-based domain adaptation algorithms typically seek to minimize the empirical estimate $\hD^\lay$ of $\D^\lay $ at each layer \cite{LongCWJ15, TzengHZSD14, GhifaryKZ14}. The empirical distribution discrepancy $\hD^\lay$ is obtained from the source and target sample sets $\{ \xis \}_{i=1}^{\Ns}$ and $\{ \xjt \}_{j=1}^{\Nt}$ as
%
\begin{equation}
\label{eq_hD2_dda}
\begin{split}
& (\hD^\lay)^2 (\fsl, \ftl) = \left \|  \frac{1}{\Ns} \sum_{i=1}^{\Ns} \fsl(\xis) -  \frac{1}{\Nt} \sum_{j=1}^{\Nt} \ftl(\xjt)  \right \|_{\Xl}^2 \\
& =  \frac{1}{\Ns^2}  \sum_{i=1}^{\Ns}  \sum_{j=1}^{\Ns} \krl(\hsl_i, \hsl_j)
 -  \frac{2}{\Ns \Nt}  \sum_{i=1}^{\Ns}  \sum_{j=1}^{\Nt} \krl(\hsl_i, \htl_j)
 +  \frac{1}{\Nt^2}  \sum_{i=1}^{\Nt}  \sum_{j=1}^{\Nt} \krl(\htl_i, \htl_j)
\end{split}
\end{equation}
%
where $\hsl_i$ and $\htl_j$ denote the source and target features in layer $\lay$ corresponding respectively to the samples $\xis$ and $\xjt$.  The second equality follows from the relations $\fsl(\xis)=\phil(\hsl_i)$ and $\ftl(\xjt)=\phil(\htl_j)$. %Burada yeni paragraf yapıyorduk, iptal ettim. 1 line kazandık
The overall distribution discrepancy between the source and the target domains defined in \eqref{eq:defn_D_MMD} is given by
%
\[
\D(\fs, \ft) = \|  E_{\xs}[\fs(\xs)] - E_{\xt}[ \ft(\xt) ] \|_\X
\]
%
following the definitions in \cref{lem_fs_ft_measble}. Its empirical estimate $\hD(\fs, \ft)$ defined in \eqref{eq:defn_hD_MMD} is then obtained as
\begin{equation}
  \label{eq_hD2_dda}
  \begin{split}
      \hD^2(\fs, \ft) &= \left \| \frac{1}{\Ns} \sum_{i=1}^{\Ns} \fs(\xis) -  \frac{1}{\Nt} \sum_{j=1}^{\Nt} \ft(\xjt) \right \|_\X^2 \\
      &= \frac{1}{\Ns^2}  \sum_{i,j=1}^{\Ns} \langle \fs(\xis), \fs(\xjs) \rangle_\X 
       - \frac{2}{\Ns \Nt}  \sum_{i=1}^{\Ns} \sum_{j=1}^{\Nt} \langle \fs(\xis), \ft(\xjt) \rangle_\X   \\
      &\quad + \frac{1}{\Nt^2}  \sum_{i,j=1}^{\Nt} \langle \ft(\xit), \ft(\xjt) \rangle_\X  \\
      &= \sum_{\lay=1}^{\numL-1}  (\hD^\lay)^2 (\fsl, \ftl) .
  \end{split}
  \end{equation}
where the last equality follows from the definition \eqref{eq_defn_inn_prod_X} of the inner product in $\X$. 

Most MMD-based deep domain adaptation networks rely on aligning the source and the target domains by minimizing the total MMD distance \eqref{eq_hD2_dda} summed over all layers \cite{WangD18, LongCWJ15, TzengHZSD14, GhifaryKZ14}. We thus consider a learning algorithm that minimizes the overall loss
%
\begin{equation}
\label{eq_obj_learning_mmd}
\begin{split}
\min_  {\fs \in \Fs,  \  \ft \in \Ft, \ \h \in  \Hs}
(1-\alpha) \hLs (\fs, \h) + \alpha \hLt (\ft, \h) 
+ \beta \sum_{\lay=1}^{\numL-1}  (\hD^\lay)^2 (\fsl, \ftl).
\end{split}
\end{equation}
%
%
Hence, the above analysis provides the bridge between the results in \cref{ssec_gen_bnd_mmd} and the current setting with MMD-based domain adaptation networks, so that the statement of \cref{thm:main_result_mmd} applies to the current problem. Before we proceed with the implications of \cref{thm:main_result_mmd}, we need two additional assumptions. 


% bu assumption in-line olsaydı 3 line kazanırdık.
\begin{assumption}
\label{assum_Lk_Leta}
The symmetric kernel $\krl(\cdot, \cdot): \Rdl \times \Rdl \rightarrow \R$ is Lipschitz continuous with constant $\Lk$ in each argument, such that
%
\begin{equation}
\label{eq_assum_kernel_lip_cont}
% \begin{split}
|  \krl( \boldsymbol{\xi}_1, \boldsymbol{\xi})  -  \krl( \boldsymbol{\xi}_2, \boldsymbol{\xi})   | \leq \Lk \| \boldsymbol{\xi}_1 - \boldsymbol{\xi}_2 \|
% \end{split}
\end{equation}
%
for all $\boldsymbol{\xi}_1, \boldsymbol{\xi}_2, \boldsymbol{\xi} \in \Rdl$.  Also, the nonlinear activation functions $\actl$ in \eqref{eq_defn_hsl_htl} are Lipschitz-continuous with constant $\Leta$, such that
%
\begin{equation}
\label{eq_Lipcont_act}
% \begin{split}
\| \actl(\mathbf{u}) - \actl(\mathbf{v}) \| \leq \Leta \, \| \mathbf{u}-\mathbf{v} \|
% \end{split}
\end{equation}
%
for all $\mathbf{u}, \mathbf{v} \in \Rdl$, for $\lay=1, \dots, \numL$. 
\end{assumption}

% bu assumption in-line olsaydı 3 line kazanırdık.
\begin{assumption}
\label{assum_bnd_act_val_op}
The nonlinear activation functions $\actl$ in \eqref{eq_defn_hsl_htl} are bounded either in value (e.g., sigmoid, softmax) or as an operator (e.g., ReLU). In the former case, we assume that there exists a constant $\Beta>0$ with
%
\begin{equation}
\label{eq_bnd_act_value}
% \begin{split}
| \actl_i(\mathbf{u})  | \leq \Beta
% \end{split}
\end{equation}
%
for all $\mathbf{u} \in \Rdl$, for $\lay=1, \dots, \numL-1$ and $i = 1, \dots, \dl$, where $\actl_i(\mathbf{u})$ denotes the $i$-th component of $\actl(\mathbf{u})$. In the latter case, we assume that there exists $\Bopeta >0$ such that
%
\begin{equation}
\label{eq_bnd_act_op}
% \begin{split}
\| \actl(\mathbf{u})  \| \leq \Bopeta  \| \mathbf{u} \|
% \end{split}
\end{equation}
%
for all $\mathbf{u} \in \Rdl$, for $\lay=1, \dots, \numL-1$.

\end{assumption}


The Lipschitz continuity condition \eqref{eq_assum_kernel_lip_cont} holds for many widely used kernels such as Gaussian kernels. As for condition \eqref{eq_Lipcont_act}, the Lipschitz constants of the commonly used rectified linear unit, softmax and softplus activation functions are derived in \cref{sec_app_lip_nonlinact}. Under these assumptions, the transformation function classes $\Fs, \Ft$ and the composite function classes $\Gs$, $\Gt$ are compact metric spaces with respect to the metrics defined in \eqref{eq_defn_dXs_dXt} and \eqref{eq_defn_ds_dt}, respectively. This compactness result (\cref{lem_Fs_Ft_HFs_HFt_comp}) is established by showing that the bounded parameter space is compact and the mapping from parameters to functions is continuous. Having established compactness, we can now characterize the covering numbers of these function classes.

To upper bound the covering numbers, we construct finite covers by discretizing the network parameter space into regular grids and leveraging the Lipschitz continuity of network components to control the induced function space distances. This analysis yields explicit covering number bounds in terms of network depth, width, and the problem-dependent constants (\cref{lem_cov_num_Fs_Ft,lem_cov_num_HFs_HFt}). From these technical results, we obtain the following characterization of the growth rates of covering numbers with network depth and width.




\begin{corollary}
\label{cor_covnum_rate}

Consider that the feature dimensions $\dl$ are such that $\dl = O(\dcom)$ for $\lay=1, \dots, \numL$, for some common network width parameter $\dcom$. Then, the rate of growth of the covering numbers for the function spaces $\N (  \Fs, \epsilon, \dXs) $, $\N (  \Ft, \epsilon, \dXt) $, $\N ( \Hs \circ \Fs, \epsilon, \ds) $, $\N ( \Hs \circ \Ft, \epsilon, \dt) $  with the width $\dcom$ and the depth $\numL$ of the network is upper bounded by 
%
\[
O\left( \left( \frac{  \numL }{\epsilon}  \right)^{\dcom^2 \numL}   (c \dcom)^{\dcom^2 \numL^2} \right)
\]
%
where $c$ denotes a constant.
%


%
\end{corollary}

\Cref{cor_covnum_rate} is proved in \cref{pf_cor_covnum_rate} of the supplement. Combining \cref{cor_covnum_rate} and \cref{thm:main_result_mmd}, we are now ready to state our main result about the sample complexity of MMD-based domain adaptation networks in \cref{thm_main_result_da_mmd} below, whose proof is presented in \cref{pf_thm_main_result_da_mmd} of the supplement. 





\begin{theorem}
\label{thm_main_result_da_mmd}

Consider a learning algorithm relying on the minimization of a loss function of the form \eqref{eq_obj_learning_mmd} via an MMD-based domain adaptation network. Assume that the classification loss function $\loss$ is bounded by a constant $\bls$ and Lipschitz continuous with respect to the first argument with constant $\Lls$.  Suppose that the source and target data distributions satisfy Assumptions \ref{assum_existence_LLs} and \ref{assum_fx_bdd_moments}. Assume also that the network parameters, activation functions  and the kernels satisfy Assumptions \ref{assum_Ax_Atheta}-\ref{assum_bnd_act_val_op}. Consider that the weight parameter $\alpha$ in the loss function is chosen such that
%
\[
\alpha
= O \left(
\left(
\frac
{\Mt \epsilon^2}
{\dcom^2 \numL \log \left( \frac{  \numL }{\epsilon} \right)  \, 
+ 
\dcom^2 \numL^2 \log(\dcom) }
\right)^{1/2}
\right)
\]
%
according to the number $\Mt$ of available labeled target samples. Then in order to bound the expected target loss with a  generalization gap of $O(\epsilon)$ as
%
\begin{equation}
\label{eq_accuracy_thm4}
\begin{split}
\Lt(\ft, \h) \leq   \hLw(\fs, \ft, \h)+ (1-\alpha) \LLs  \hD(\fs, \ft) + (1-\alpha) \LLs \epsilon + \epsilon,
\end{split}
\end{equation}
%
the sample complexities in terms of the number $\Ms$ of labeled source samples, the number $\Ns$ of all (labeled and unlabeled) source samples, and the number $\Nt$  of all target samples are upper bounded by
%
\begin{equation}
O\left(
\frac{\dcom^2 \numL \log \left(\frac{\numL}{\epsilon}\right) 
+ \dcom^2 \numL^2 \log(\dcom) }
{\epsilon^2}
\right) . 
\end{equation}
\end{theorem}




\cref{thm_main_result_da_mmd} shows that sample complexities $\Ms$, $\Ns$, and $\Nt$ must increase at rate $O(\dcom^2 \numL^2)$ as network depth $\numL$ and width $\dcom$ increase (ignoring logarithmic terms), indicating quadratic growth with network size to prevent overfitting.\footnote{The assumption of the existence of constants $\bls$ and $\Lls$ is satisfied in many settings; we derive these for cross-entropy loss in Appendix \ref{sec_app_crossent}.} For limited labeled target samples $\Mt$, the weight $\alpha$ must shrink at rate $\alpha=O(\sqrt{\Mt})$ to avoid overfitting, and similarly at rate $\alpha=O((\dcom \numL)^{-1})$ as network size grows. Sample sizes scale as $O(\epsilon^{-2})$ for an $O(\epsilon)$ bound on loss difference. 

\clearpage
\subsection{Adversarial domain adaptation networks}
\label[subsection]{ssec_adv_da_net}

\begin{figure}[t]
  \centering
  \centerline{\includegraphics[width=13.0cm]{figures/illus_ddan_network.pdf}}
  \caption{Illustration of adversarial domain adaptation networks}
  \label{fig_illus_ddan_network}
\end{figure}

In this section, we extend our results to analyze the sample complexity of adversarial domain adaptation networks.
% \footnote{Adversarial models have been widely used since \cite{GaninUAGLLML16,TzengHSD17,LongC0J18} and applied to various problems \cite{SinghalWRK23}.} 
Domain-adversarial neural networks aim to compute domain-invariant representations $\fs: \Xs \rightarrow \X$, $\ft: \Xt \rightarrow \X$ through a feature extractor network, followed by a label predictor $\h: \X \rightarrow \Y$ (\cref{fig_illus_ddan_network}). The domain-invariance of the learnt features is ensured by a domain discriminator network trained to distinguish source from target features. The feature extractor and discriminator are trained adversarially: the extractor learns representations indistinguishable to the discriminator. The domain discriminator $\ddan: \X \rightarrow \R$ minimizes the domain discrimination loss
\(
\Ls_\dom( \fs, \ddan) + \Lt_\dom( \ft, \ddan)
\)
where
\(
\Ls_\dom( \fs, \ddan)=E[\loss_\dom(\ddan \circ \fs(\xs), \ydoms)]
\), and
\(
\Lt_\dom( \ft, \ddan) = E[ \loss_\dom( \ddan \circ \ft(\xt), \ydomt )]
\)
%
respectively denote the expected domain discrimination losses in the source and the target domains;  
$\loss_\dom: \R \times \R \rightarrow [0, \infty)$ is a domain discrimination loss function; and $\ydoms,  \ydomt \in \R$ denote the domain labels of the source and the target domains. It is common practice to set the domain discrimination loss $\loss_\dom$ as a logarithmic penalty on the deviation between the estimated domain labels and the true domain labels $\ydoms =0$, $\ydomt =1$ as \cite{GaninUAGLLML16, TzengHSD17, LongC0J18}:
\begin{equation}
\label{eq_log_penalty_domainloss}
\begin{split}
\loss_\dom( \ddan \circ \fs(\xs), \ydoms ) &= - \log(1-\ddan \circ \fs(\xs))\\
\loss_\dom( \ddan \circ \ft(\xt), \ydomt ) &= - \log(\ddan \circ \ft(\xt)). 
\end{split}
\end{equation}
Meanwhile, the feature extractor network is trained to maximize the domain classification loss so that the learnt features are domain-invariant, leading to the overall optimization problem 
\begin{equation}
\label{eq_ddan_obj_func}
\begin{split}
\min_  {\fs,  \ft, \h, \ddan}
(1-\alpha) \hLs (\fs, \h) + \alpha \hLt (\ft, \h) 
- \beta \big( \hLs_\dom (\fs, \ddan) +\hLt_\dom( \ft, \ddan) \big)
\end{split}
\end{equation}
%
where $\hLs, \hLt$ denote the empirical source and target classification losses defined in \eqref{eq_emp_cl_loss}. Here  $\hLs_\dom,  \hLt_\dom$ are the empirical domain discrimination losses given by
%
\begin{align*}
\hLs_\dom( \fs, \ddan) &= \frac{1}{\Ns} \sum_{i=1}^{\Ns}  \loss_\dom( \ddan \circ \fs(\xis), \ydoms_i )&
\hLt_\dom( \ft, \ddan) &=  \frac{1}{\Nt} \sum_{j=1}^{\Nt} \loss_\dom( \ddan \circ \ft(\xjt), \ydomt_j  ).
\end{align*}
%
where $\ydoms_i $ and $\ydomt_j $ respectively denote the domain labels of the source samples $\xis $ and the target samples $\xjt$. 
%


In order to study domain-adversarial network models within our framework, we consider that the transformations $\fs, \ft$ are given by the feature representations at layer $L-1$ of the feature extractor network. The corresponding function spaces are then 
%
\begin{equation}
\label{eq_Fs_defn}
\begin{split}
\Fs &= \{ \fs: \Xs \rightarrow \R^{\dLm} \, | \,  \fs(\xs)= \hsLm_{\Thetas}(\xs), \ |\Thetasl_{ij}|  \leq \Bnet, \forall i, j\}\\
\Ft &= \{ \ft: \Xt \rightarrow \R^{\dLm} \, | \,  \ft(\xt)= \htLm_{\Thetat}(\xt), \ |\Thetatl_{ij}|  \leq \Bnet, \forall i, j\}.  
\end{split}
\end{equation}
%
Similarly, the hypotheses $\h \circ \fs$ and $\h \circ \ft$ are given by the output of the last layer $L$ 
%
$
\h \circ \fs(\xs)= \hsL(\xs)
$, and 
$
\h \circ \ft(\xt)= \htL(\xt)
$ %
%
with the function spaces $\Hs \circ \Fs$ and $\Hs \circ \Ft$ defined\footnote{Note that, the definitions of the function spaces $\Fs, \Ft$ in this section are different from those in \cref{ssec_samp_comp_mmd_net}, as they take different roles between MMD-based and adversarial  networks. Nevertheless, the composite function spaces $\Gs = \Hs \circ \Fs$ and $\Gt = \Hs \circ \Ft$ in this section are the same as those of \cref{ssec_samp_comp_mmd_net}, since the functions $\gs, \gt$ are defined through the classification layer output in both the MMD-based and the adversarial settings.} in \eqref{eq_Gs_Gt_defn_dl}. Here, the features between layers $\lay-1$ and $\lay$ are related as in \eqref{eq_defn_hsl_htl} through the network parameters $\Wsl, \Wtl, \bsl, \btl$ and the nonlinear activation functions $\actl$. \footnote{\textcolor{red}{Buraya bir ayar verilecek.} While feature extractor networks typically consist of several convolutional layers followed by fully connected layers in many common architectures \cite{SinghalWRK23}; in domain adaptation applications it is a common strategy to adopt convolutional layer weights from pretrained networks or to train or fine-tune them using only source data \cite{TzengHSD17}. Therefore, we leave the training of convolutional layers out of the scope of our analysis. We consider the input source and target samples $\xs, \xt \in \R^{\dz}$ to be the response generated at the output of the convolutional network common between the two domains as illustrated in \cref{fig_illus_ddan_network} and focus on the action of the fully connected layers of the feature extractor networks.}


The domain discriminator network typically consists of several fully connected layers \cite{GaninUAGLLML16, TzengHSD17}. Denoting the weight parameters of these layers as $\Wldan \in \R^{\dlddan \times \dlmddan}$, $\bldan \in \R^{\dlddan}$, the relation between the responses $\hidlmdan \in \R^{\dlmddan},  \hidldan \in \R^{\dlddan}$ at layers $\lay-1$ and $ \lay$ is given by
%
$
\hidldan= \actldan( \Wldan \hidlmdan + \bldan)
$
%
for $\lay=1, \dots, \numLdan $, where $\numLdan $ denotes the number of layers and $\actldan: \R^{\dlddan} \rightarrow \R^{\dlddan}  $ denotes the activation function of the domain discriminator network at layer $\lay$.  Here, the input $\hidzdan $  to the domain discriminator network corresponds to the outputs $\hsLm, \htLm$ of the feature extractor networks. The domain discriminator output is then given by
%
$
\ddan \circ \fs(\xs) = \hidKdan (\xs)
$, and 
$
\ddan \circ \ft(\xt) = \hidKdan (\xt)
$ 
%
for the source and the target domains, where the dimension of the output layer of the domain discriminator is $\dKddan=1$. Still using \cref{assum_Ax_Atheta} and extending it to the domain discriminator network as well, we define the function class of domain discriminators with bounded network weights as 
\begin{equation}
\label{eq_Dspace_defn}
\begin{split}
\Dspace &= \{ \ddan: \R^{\dLm} \rightarrow \R \ | \ 
\ddan(\hidzdan)=\hidKdan , 
\
| ({\Wldan})_{ij}|  \leq \Bnet, \
 |({\bldan})_{i}|   \leq \Bnet,
  \forall i, j\}  .
\end{split}
\end{equation}
%

Provided that the adversarial domain adaptation network is well-trained, the mappings $\fs(\xs)$, $\ft(\xt)$ specialize in the extraction of domain-invariant features such that the domain discriminator cannot distinguish between the source and the target samples. The discriminator outputs  $\ddan \circ \fs(\xs)$ and $\ddan \circ \ft(\xt) $ then take similar values. Based on this observation, we build our analysis on the following definition of the distribution distance
%
$$%
\Ddan(\fs, \ft) \triangleq \left | E[\ddan \circ \fs(\xs) ]  - E [\ddan \circ \ft(\xt) ]  \right |.
$$%
%
The distribution distance $\Ddan(\fs, \ft)$  measures how well the source and target distributions are aligned once they are mapped to the shared feature space by the mappings $\fs$ and $\ft$. Note that the above definition of the distribution distance $\Ddan(\fs, \ft)$ depends also on the domain discriminator $\ddan$. We make the following assumption about the domain discriminator.

\begin{assumption}
\label{assum_ddan_bdd}
The domain discriminator output is bounded, i.e., there exists a constant $\Bddan>0$ such that
$|\ddan(\hidzdan)|=|\hidKdan| \leq \Bddan $ for all $\hidzdan \in \R^{\dLm}$.
\end{assumption}
 
Note that \cref{assum_ddan_bdd} is satisfied for many domain-adversarial networks, as the activation function $\actKdan $ of the final domain discriminator layer is often selected as a bounded function such as the sigmoid  \cite{GaninUAGLLML16} or the softmax function \cite{TzengHDS15}. Let us denote the composition of the domain discriminator and the feature extractor as $ \vs(\xs) \triangleq \ddan \circ \fs (\xs) \, \vt(\xt) \triangleq \ddan \circ \ft (\xt)$,
and the corresponding function spaces as $
\Vs &= \Dspace \circ \Fs = 
\{
\vs: \vs= \ddan \circ \fs,  \ddan \in \Dspace, \fs \in \Fs
\}$, and $
\Vt &= \Dspace \circ \Ft = 
\{
\vt: \vt= \ddan \circ \ft,  \ddan \in \Dspace, \ft \in \Ft
\}$.


In order to study the sample complexity of adversarial domain adaptation networks, we characterize the deviation between the expected distribution distance $\Ddan(\fs, \ft)$ and its finite-sample estimate 
\begin{equation*}
  \hDdan(\fs, \ft) = \left | \frac{1}{\Ns} 
  \sum_{i=1}^{\Ns} \ddan \circ \fs(\xis)   - 
   \frac{1}{\Nt}     \sum_{j=1}^{\Nt} \ddan \circ \ft(\xjt)  
 \right |
\end{equation*}
 in \cref{lem_ddan_hddan_dev}, which is the counterpart of \cref{lem:unif_bnd_D_hD} in the domain-adversarial setting.
% \footnote{The proof is presented in \cref{pf_lem_ddan_hddan_dev}.} 
Before stating the main result of this section, we formalize the following conditions. 

\begin{assumption}
\label{assum_actddan_cont_Lip}
The activation functions $\actl (\cdot)$ for layers $\lay=1, \dots, \numL$ and the activation functions $\actldan (\cdot)$ for layers  $\lay=1, \dots, \numLdan$ are continuous and also Lipschitz-continuous with constant $\Leta$, such that
%
$
\| \actl(\mathbf{u}) - \actl(\mathbf{v}) \| \leq \Leta \, \| \mathbf{u}-\mathbf{v} \|
$ 
 for all $\mathbf{u}, \mathbf{v} \in \Rdl$, for $\lay=1, \dots, \numL$ and
$
\| \actldan(\mathbf{u}) - \actldan(\mathbf{v}) \| \leq \Leta \, \| \mathbf{u}-\mathbf{v} \|
$
%
for all $\mathbf{u}, \mathbf{v} \in \R^{\dlddan}$, for $\lay=1, \dots, \numLdan$. 
\end{assumption}


% \begin{figure}[t]
% \begin{center}
%      \subfloat[Poor alignment]
%        {\label{fig:illus_domain_ddan_a}\includegraphics[width = 0.5\textwidth]{figures/illus_ddan_bad_align.pdf}}
%        \hspace{1cm}
%      \subfloat[Proper alignment]
%        {\label{fig:illus_domain_ddan_b}\includegraphics[height=4cm]{figures/illus_ddan_good_align.pdf}}
%  \end{center}
%  \vspace{-0.5cm}
%  \caption{Illustration of \cref{assum_existence_LLsdan}. Red and blue colors represent two different classes in the source and target domains. In (a), the two domains are poorly aligned by the mappings $\fs$ and $\ft$, therefore, the algorithm learns a domain discriminator $\ddan $ that can separate the two domains well. The domain distance $\Ddan(\fs, \ft)$ is then high, and consequently, there may exist hypotheses $\h$ yielding a small loss in one domain and a large loss in the other domain. In (b), the domains are well-aligned and the domain distance $\Ddan(\fs, \ft)$ is small. The source and target losses are then similar for any hypothesis $\h$.}
%  \label{fig_illus_domain_ddan}
% \end{figure}

\begin{figure}[t]
  \centering
  \subfloat[Poor alignment]{\label{fig:illus_domain_ddan_a}\includegraphics[width=0.35\textwidth]{figures/illus_ddan_bad_align.pdf}}
  \hspace{0.1\textwidth}
  \subfloat[Proper alignment]{\label{fig:illus_domain_ddan_b}\includegraphics[width=0.35\textwidth]{figures/illus_ddan_good_align.pdf}}
  \caption{Illustration of \cref{assum_existence_LLsdan}. Red and blue colors represent two different classes in the source and target domains. In (a), the two domains are poorly aligned by the mappings $\fs$ and $\ft$, therefore, the algorithm learns a domain discriminator $\ddan $ that can separate the two domains well. The domain distance $\Ddan(\fs, \ft)$ is then high, and consequently, there may exist hypotheses $\h$ yielding a small loss in one domain and a large loss in the other domain. In (b), the domains are well-aligned and the domain distance $\Ddan(\fs, \ft)$ is small. The source and target losses are then similar for any hypothesis $\h$.}
  \label{fig_illus_domain_ddan}
  \end{figure}

\begin{assumption}
\label{assum_bnd_act_val_op_ddan}
The nonlinear activation functions $\actldan$  are bounded either in value or as an operator, for $\lay=1, \dots, \numLdan-1$. In the former case, there exists a constant $\Beta>0$ with
% \begin{equation}
% \label{eq_bnd_act_value_ddan}
% \begin{split}
$| ({\actldan})_i(\mathbf{u)}  | \leq \Beta $
% \end{split}
% \end{equation}
for all $\mathbf{u} \in \R^{\dlddan}$, where $({\actldan})_i(\mathbf{u})$ denotes the $i$-th component of $\actldan(\mathbf{u})$. In the latter case,  there exists $\Bopeta >0$ such that
%
% \begin{equation}
% \label{eq_bnd_act_op_ddan}
% \begin{split}
$\| \actldan(\mathbf{u})  \| \leq \Bopeta  \| \mathbf{u} \| $
% \end{split}
% \end{equation}
%
for all $\mathbf{u} \in \R^{\dlddan}$. 
\end{assumption}

{Note that \cref{assum_actddan_cont_Lip} is an adaptation of the conditions in Assumptions \ref{assum_krl_actl_cont} and \ref{assum_Lk_Leta} to the domain-adversarial setting in consideration.}
{Similarly, \cref{assum_bnd_act_val_op_ddan} simply adapts the condition in \cref{assum_bnd_act_val_op} to the domain discriminator network.} We lastly make the following assumption about the link between the distribution distance and the deviation between the source and target losses.

\begin{assumption}
\label{assum_existence_LLsdan}
There exists a constant $\LLsdan>0$ such that, for the domain discriminator $\ddan \in \Dspace $ learnt by the algorithm, we have
%
\begin{equation*}
% \label{eq:Lipsc_reg_loss_ddan}
| \Ls(\fs, \h) - \Lt(\ft, \h) | \leq \LLsdan \, \Ddan(\fs, \ft)
\end{equation*}
%
for any transformations $\fs \in \Fs$, $\ft \in \Ft$, and any hypothesis $\h \in \Hs$. 
\end{assumption} 
%
\cref{assum_existence_LLsdan} is the counterpart of \cref{assum_existence_LLs} in the context of adversarial domain adaptation networks, which is illustrated in \cref{fig_illus_domain_ddan}. The assumption asserts that the source and the target distributions be related in such a way that, when efficiently aligned via the feature mappings $\fs$ and $\ft$ so as to minimize the domain discrepancy $\Ddan(\fs, \ft)$, the classification losses arising in the source and the target domains are also comparable. \footnote{Note that the assumption is not limited to the ideal scenario where the domains are well-aligned: In  case of poor alignment,  $\Ddan(\fs, \ft)$ may be high, possibly leading to significantly different losses in the two domains. We, however, assume that the domain discriminator network is sufficiently well-trained; i.e., the learnt discriminator $\ddan $ is able to distinguish between the source and target domains if the mappings $\fs$ and $\ft$ result in poor feature alignment.}



We can now state our main result about the sample complexity of adversarial domain adaptation networks. 


\begin{theorem}
\label{thm_main_result_dann}

Consider a learning algorithm relying on the minimization of a loss function of the form \eqref{eq_ddan_obj_func} via an adversarial domain adaptation network. Assume that the classification loss function $\loss$  is bounded by a constant $\bls$ and Lipschitz continuous with respect to the first argument with constant $\Lls$.  Suppose that the source and target data distributions satisfy \cref{assum_existence_LLsdan} and the network parameters and activation functions  satisfy \cref{assum_Ax_Atheta} and \crefrange{assum_bnd_act_val_op}{assum_bnd_act_val_op_ddan}. 

Let the feature dimensions be such that $\dl = O(\dcom)$ for $\lay=1, \dots, \numL$ and $\dlddan = O(\dcom)$ for $\lay=1, \dots, \numLdan$ for some common width parameter $\dcom$. Consider that the weight parameter $\alpha$ in the loss function is chosen such that
%
\begin{equation}
\label{eq_alpha_exrp_thm_ddan}
\begin{split}
\alpha
= O \left(
\left(
\frac
{\Mt \epsilon^2}
{\dcom^2 \numL \log \left( \frac{  \numL }{\epsilon} \right)  \, 
+ 
\dcom^2 \numL^2 \log(\dcom) }
\right)^{1/2}
\right)
\end{split}
\end{equation}
%
according to the number $\Mt$ of available labeled target samples. Then, in order to bound the expected target loss with a generalization gap of $O(\epsilon)$ as
%
\begin{equation}
\label{eq_accuracy_thm_ddan}
\begin{split}
\Lt(\ft, \h) \leq   \hLw(\fs, \ft, \h)+ (1-\alpha) \LLsdan  \hDdan(\fs, \ft) + (1-\alpha) \LLsdan \epsilon + \epsilon,
\end{split}
\end{equation}
%
the sample complexities in terms of the number $\Ms$ of labeled source samples, the number $\Ns$ of all (labeled and unlabeled) source samples, and the number $\Nt$  of all target samples are upper bounded by
%
\begin{equation}
\label{eq_sample_complexity_thm_ddan}
\begin{split}
\Ms &= O\left(
\frac{\dcom^2 \numL \log \left(\frac{\numL}{\epsilon}\right) 
+ \dcom^2 \numL^2 \log(\dcom) }
{\epsilon^2}
\right) \\
\Ns, \Nt &= 
O\left(
\frac{\dcom^2 (\numL+\numLdan) \log \left(\frac{\numL+ \numLdan}{\epsilon}\right) 
+ \dcom^2 (\numL+\numLdan)^2 \log(\dcom) }
{\epsilon^2}
\right).
\end{split}
\end{equation}
%
\end{theorem}
The proof of \cref{thm_main_result_dann} is presented in \cref{pf_thm_main_result_dann}. The findings of  \cref{thm_main_result_dann} on the sample complexity of domain-adversarial networks are in line with those of \cref{thm_main_result_da_mmd}, which studied MMD-based networks. The optimal choice for the weight parameter $\alpha$ scales as $O(\sqrt{\Mt})$ as the number of labeled target samples varies. In order to prevent overfitting, $\Ms$ must increase at rate $\Ms=O(\dcom^2 \numL^2)$ with $\dcom$ and $\numL$, which indicates that the number of labeled source samples must increase quadratically with the width $\dcom$  and the depth $\numL$ of the feature extractor network, ignoring the logarithmic factors. Likewise, the number  of source and target samples $\Ns$ and $\Nt$  must also increase at a quadratic rate $O(\dcom^2 (\numL+\numLdan)^2)$ with the width $\dcom$ and the depth $\numL + \numLdan$ of the combination of feature extractor and domain discriminator networks, in order to avoid overfitting to the empirical domain discrimination loss of training samples. Similarly to the result in \cref{thm_main_result_da_mmd}, for the difference between the expected target loss and the sum of the empirical losses to be bounded by an amount of $O(\epsilon)$, the number of samples $\Ms, \Ns, \Nt$ must scale at rate $O(\epsilon^{-2})$.  
While we analyze single-layer label predictors as in \cref{fig_illus_ddan_network} following common practice, our results extend to multi-layer predictors with depth $P$ by replacing $\numL$ with $\numL+P$ in the complexity bounds. The optimal choice of the weight parameter $\alpha$ in \eqref{eq_alpha_exrp_thm_ddan} can similarly be obtained by replacing the number of layers $\numL$ with $\numL+P$ in this case.\footnote{This is due to the fact that our analysis is based on the covering numbers of the function spaces $\Gs, \Gt$ and $\Vs, \Vt$, where $\N(\Gs, \epsilon, \ds)$,  $\N(\Gt, \epsilon, \dt)$  depend on only the total number of layers in the cascade of the feature extractor and the label predictor networks, and $\N(\Vs, \epsilon, \dVs)$,   $\N(\Vt, \epsilon, \dVt)$ depend only on the total number of layers in the cascade of the feature extractor and the domain discriminator networks.}
% \begin{remark} 
\footnote{In our analysis, we have considered the label predictor network to consist of a single layer as illustrated in \cref{fig_illus_ddan_network}, as common practice in adversarial domain adaptation networks.  Nevertheless, it is straightforward to adapt our results to the case where the label predictor network consists of more than one layer. This is due to the fact that our analysis is based on the covering numbers of the function spaces $\Gs, \Gt$ and $\Vs, \Vt$, where $\N(\Gs, \epsilon, \ds)$,  $\N(\Gt, \epsilon, \dt)$  depend on only the total number of layers in the cascade of the feature extractor and the label predictor networks, and $\N(\Vs, \epsilon, \dVs)$,   $\N(\Vt, \epsilon, \dVt)$ depend only on the total number of layers in the cascade of the feature extractor and the domain discriminator networks.  Denoting the depth of the label predictor network as $P$ in this alternative setting, the resulting sample complexities would be obtained as  $\Ms=O(\dcom^2 (\numL+P)^2)$, and $\Ns, \Nt = O(\dcom^2 (\numL+\numLdan)^2)$. The optimal choice of the weight parameter $\alpha$ in \eqref{eq_alpha_exrp_thm_ddan} can similarly be obtained by replacing the number of layers $\numL$ with $\numL+P$ in this case.}
% \end{remark}

% AI generated, a bit shorter version:
% The proof of the \cref{thm_main_result_dann} is in \cref{pf_thm_main_result_dann}. The findings of \cref{thm_main_result_dann} on the sample complexity of domain-adversarial networks are in line with those of \cref{thm_main_result_da_mmd}: the optimal weight $\alpha$ scales as $O(\sqrt{\Mt})$; to prevent overfitting, $\Ms$ must increase at rate $\Ms=O(\dcom^2 \numL^2)$ (quadratically with width $\dcom$ and depth $\numL$ of the feature extractor), while $\Ns$ and $\Nt$ must increase at rate $O(\dcom^2 (\numL+\numLdan)^2)$ (quadratically with the combined depth $\numL + \numLdan$ of feature extractor and discriminator networks); and sample sizes scale as $O(\epsilon^{-2})$ for an $O(\epsilon)$ bound on the loss difference.  



\clearpage
\section{Experimental results}
\label{sec:exp_results}

In this section, we present experimental results for the verification of the proposed generalization bounds. We first study the generic bounds using shallow classifier models, then examine the sample complexity of domain-adaptive neural networks. Complete experimental details are provided in \cref{sec_detailed_exp_results} of the supplement.


\subsection{General domain alignment methods} 
\label{ssec_exp_gen_da}

We validate our findings on a synthetic data set with two classes.\footnote{Source and target data are generated by applying different geometric transformations to 400 samples drawn from the standard normal distribution in $\R^2$. We emulate a setting where the transformations $\fs$ and $\ft$ are learnt with some estimation error $\tau$. The classifier is a regularized ridge regression trained in the common domain. Target misclassification rates are evaluated over 1000 test samples. Complete setup is provided in \cref{ssec_exp_gen_da_detailed}.}

\cref{fig:toy_error}(a) shows the variation of the target misclassification rate with the number $\Mt$ of labeled target samples for different values of $\alpha$. The decay in the target error is consistent with the rate $O(\sqrt{1/\Mt})$ predicted by \cref{thm:gen_defect_target}, as confirmed by the fitted theoretical curves. We also observe that large $\Mt$ values favor larger $\alpha$, while smaller $\Mt$ values require smaller $\alpha$, supporting the scaling $\alpha = O(\sqrt{\Mt})$. \cref{fig:toy_error}(b) illustrates that the misclassification rate increases approximately linearly with the transformation estimation error $\tau$, which is proportional to $\D(\fs, \ft)$, confirming the prediction of \cref{thm:gen_defect_target} that the target loss increases proportionally to the distribution distance.

We next experiment on the MIT-CBCL face data set \cite{MITCBCL} (\cref{fig:face_dataset}).\footnote{The dataset contains 3240 synthetic face images of 10 subjects rendered under 36 illumination conditions and 9 poses. We use Pose 1 (frontal) as source and Poses 2, 5, 9 as targets in separate trials. Domain alignment uses the PCA-based method of \cite{FernandoHST13} and an SVM classifier. Complete setup in \cref{ssec_exp_gen_da_detailed}.} \cref{fig:mit_error} shows that misclassification rates are effectively reduced with increasing labeled samples, at rates consistent with $O(\sqrt{1/\Mt})$ and $O(\sqrt{1/\Ms})$, as predicted by our theory. The fitted theoretical curves closely match the experimental data.


\subsection{Domain-adaptive neural networks} 
\label{ssec_exp_dan}

We experimentally verify the sample complexity results of \cref{thm_main_result_da_mmd,thm_main_result_dann} using MNIST $\rightarrow$ MNIST-M experiments \cite{mnist,mnistm}.\footnote{MNIST (60000 images) serves as source and MNIST-M (59000 colored background images) as target. Networks are trained with varying numbers of labeled and unlabeled samples. Hyperparameters are chosen to keep the network in the overfitting regime, where target accuracy decreases as network complexity grows for fixed sample sizes, enabling the characterization of sample complexity. Target accuracy vs.\ network complexity curves are obtained via linear extrapolation; sample complexity is characterized by identifying the minimum sample sizes required to maintain reference accuracy levels as complexity increases. Complete details in \cref{ssec_exp_dan_detailed}.}

\subsubsection{MMD-based domain adaptation networks}
\label{ssec_exp_mmd_dan}

We adopt the MMD-based architecture from \cite{LongCWJ15}, building on our previous experimental study \cite{KaracaAAAAUV23}.\footnote{The network begins with convolutional layers followed by fully connected MMD layers with coupled parameters between domains, and is trained with batch normalization after each layer. The network minimizes cross-entropy loss weighted by $(1-\alpha)$ and $\alpha$ for source and target samples, plus $\beta$ times the MMD distance across all layers. The parameter $\beta$ is chosen inversely proportional to $\numL$ to prevent the MMD term from dominating. Training is conducted with SGD (learning rate 0.001, momentum 0.9, batch size 512). Complete implementation details and hyperparameter values are provided in \cref{ssec_exp_dan_detailed}.} \cref{fig_mmd_results_L} presents the sample complexity with respect to network depth: the left panels show the decrease in target accuracy as $\numL$ increases for different $\Ms$ and $\Ns$ values, while the right panels plot the minimum sample sizes needed to achieve reference accuracy levels, with fitted quadratic polynomials confirming $\Ms, \Ns=O(\numL^2)$. \cref{fig_mmd_results_d} demonstrates analogous quadratic growth $\Ms=O(\dcom^2)$ with respect to the width parameter $\dcom$, which scales the width of all MMD layers. These results are consistent with the predictions of \cref{thm_main_result_da_mmd}.

\cref{fig_mmd_results_alpha} validates the theoretical prediction for the optimal weight parameter. The target accuracy exhibits a non-monotonic variation with $\alpha$, and the optimal value $\alpha_{opt}$, identified via polynomial fitting for each $\Mt$, scales as $\alpha_{opt}=O(\sqrt{\Mt})$, in agreement with \cref{thm_main_result_da_mmd}.


\subsubsection{Adversarial domain adaptation networks}
\label{exp_adv_da}

We adopt the domain-adversarial architecture from \cite{GaninUAGLLML16}, adapted for our semi-supervised setting.\footnote{The feature extractor is trained adversarially against a domain discriminator using the objective in \cref{ssec_exp_dan_detailed}, with negative log likelihood for both classification and domain discrimination losses, and $\fs=\ft=f$. The original implementation \cite{fungtion_DANN_py3} was modified to (i) support labeled target samples in the classification loss, and (ii) systematically scale the network capacity via $\numL$ and $\dcom$. When studying $\Ms$, the feature extractor and label predictor depths are set equal as $\numL$; for $\Ns$, the feature extractor and domain discriminator depths are equated. The width parameter $\dcom$ scales both convolutional channels and fully connected layer widths proportionally. Training uses Adam (learning rate 0.001, batch size 128, 100 epochs). Complete details in \cref{ssec_exp_dan_detailed}.}

Figures \ref{fig_adv_results_L}-\ref{fig_adv_results_d} present the sample complexity analysis. The results confirm \cref{thm_main_result_dann}: required sample sizes grow quadratically as $\Ms, \Ns=O(\numL^2)$ and $\Ms, \Ns = O(\dcom^2)$ with respect to network depth and width. \cref{fig_adv_results_alpha} further validates that the optimal weight parameter scales as $\alpha_{opt}=O(\sqrt{\Mt})$, consistently across different $\Ms$ values.



\begin{figure}[t]
  \begin{center}
       \subfloat[]
         {\label{fig:toy_error_Mt}\includegraphics[width = 0.4\textwidth]{figures/toy_error_Mt_alpha.pdf}}
       \subfloat[]
         {\label{fig:toy_error_tau}\includegraphics[width = 0.4\textwidth]{figures/toy_error_tau_alpha.pdf}}
   \end{center}
   \caption{Variation of the target error on synthetical data with (a) Number of labeled target samples, (b) Distribution distance after transformation. Solid lines indicate experimental data and dashed lines represent theoretical rates of variation.}
   \label{fig:toy_error}
\end{figure}


\begin{figure}[t]
  \centering
  \centerline{\includegraphics[width=7.0cm]{figures/face_dataset.pdf}}
  \caption{Sample images from the MIT-CBCL face data set for four different subjects, rendered respectively under poses 1, 2, 5, and 9 for various illumination conditions. }
  \label{fig:face_dataset}
\end{figure}


\begin{figure}[t]
\begin{center}
     \subfloat[]
       {\label{fig:mit_error_Mt}\includegraphics[width = 0.4\textwidth]{figures/mit_error_Mt.pdf}}
     \subfloat[]
       {\label{fig:mit_error_Ms}\includegraphics[width = 0.4\textwidth]{figures/mit_error_Ms.pdf}}
 \end{center}
 \caption{Variation of the target error on MIT-CBCL face data with (a) Number of labeled target samples, (b) Number of labeled source samples. Solid lines indicate experimental data and dashed lines represent theoretical rates of variation.}
 \label{fig:mit_error}
\end{figure}



\clearpage


%% Ms, Ns vs. L for MMD
\begin{figure}[t]
\centering
\subfloat[]%
  {\label{fig_mmd-layer-Ms-linefit}\includegraphics[width = 0.4\textwidth]{figures/mmd-layer-Ms-linefit.pdf}}
  % \hspace{0.1\textwidth}
\subfloat[]%
  {\label{fig_mmd-layer-Ms-quadprog}\includegraphics[width = 0.4\textwidth]{figures/mmd-layer-Ms-quadprog.pdf}}\\
\subfloat[]%
  {\label{fig_mmd-layer-Ns-linefit}\includegraphics[width = 0.4\textwidth]{figures/mmd-layer-Ns-linefit.pdf}}
  % \hspace{0.1\textwidth}
\subfloat[]%
  {\label{fig_mmd-layer-Ns-quadprog}\includegraphics[width = 0.4\textwidth]{figures/mmd-layer-Ns-quadprog.pdf}}
\caption{Sample complexity with respect to depth $\numL$ for MMD-based networks \cite{LongCWJ15}. Left panels show target accuracy variation with $\numL$ at different sample sizes. Right panels show quadratic growth $\Ms, \Ns=O(\numL^2)$.}
\label{fig_mmd_results_L}
\end{figure}

%% Ms vs. d for MMD
\begin{figure}[t!]
\begin{center}
     \subfloat[]
       {\label{fig_mmd-dim-Ms-linefit}\includegraphics[width = 0.4\textwidth]{figures/mmd-dim-Ms-linefit.pdf}}
     \subfloat[]
       {\label{fig_mmd-dim-Ms-quadprog}\includegraphics[width = 0.4\textwidth]{figures/mmd-dim-Ms-quadprog.pdf}}
%
 \end{center}
 \caption{Sample complexity with respect to width $\dcom$ for MMD-based networks. Quadratic growth $\Ms=O(\dcom^2)$ confirmed.}
 \label{fig_mmd_results_d}
\end{figure}



\clearpage

\begin{figure}[t]
  \begin{center}
       \subfloat[]
         {\label{fig_adv-layer-Ms-linefit}\includegraphics[width = 0.4\textwidth]{figures/adv-layer-Ms-linefit.pdf}}
       \subfloat[]
         {\label{fig_adv-layer-Ms-quadprog}\includegraphics[width = 0.4\textwidth]{figures/adv-layer-Ms-quadprog.pdf}}\\
        \subfloat[]
         {\label{fig_adv-layer-Ns-linefit}\includegraphics[width = 0.4\textwidth]{figures/adv-layer-Ns-linefit.pdf}}
       \subfloat[]
         {\label{fig_adv-layer-Ns-quadprog}\includegraphics[width = 0.4\textwidth]{figures/adv-layer-Ns-quadprog.pdf}}
  \end{center}
   \caption{Sample complexity with respect to depth $\numL$ for adversarial networks. Left: accuracy vs depth. Right: quadratic growth $\Ms, \Ns=O(\numL^2)$.}
   \label{fig_adv_results_L}
\end{figure}

\begin{figure}[t!]
\begin{center}
     \subfloat[]
       {\label{fig_mmd-optalpha_quadfit_Ms234}\includegraphics[width = 0.4\textwidth]{figures/mmd-optalpha-quadfit_Ms234.pdf}}
     \subfloat[]
       {\label{fig_mmd-optalpha-asqrtx_fit_den_Mss}\includegraphics[width = 0.4\textwidth]{figures/mmd-optalpha-asqrtx_fit_den_Mss.pdf}}
 \end{center}
 \caption{Optimal weight parameter for MMD-based networks. (a) Target accuracy variation with $\alpha$ shows non-monotonic behavior. (b) Optimal $\alpha_{opt}$ scales as $O(\sqrt{\Mt})$.}
 \label{fig_mmd_results_alpha}
\end{figure}


\clearpage

\begin{figure}[t]
\begin{center}
     \subfloat[]
       {\label{fig_adv-dcm-Ms-linefit}\includegraphics[width = 0.4\textwidth]{figures/adv-dcm-Ms-linefit.pdf}}
     \subfloat[]
       {\label{fig_adv-dcm-Ms-quadprog}\includegraphics[width = 0.4\textwidth]{figures/adv-dcm-Ms-quadprog.pdf}}\\
            \subfloat[]
       {\label{fig_adv-dcm-Ns-linefit}\includegraphics[width = 0.4\textwidth]{figures/adv-dcm-Ns-linefit.pdf}}
     \subfloat[]
       {\label{fig_adv-dcm-Ns-quadprog}\includegraphics[width = 0.4\textwidth]{figures/adv-dcm-Ns-quadprog.pdf}}
\end{center}
 \caption{Sample complexity with respect to width $\dcom$ for adversarial networks. Left: accuracy vs width. Right: quadratic growth $\Ms, \Ns = O(\dcom^2)$.}
 \label{fig_adv_results_d}
\end{figure}

\begin{figure}[ht]
\begin{center}
     \subfloat[]
       {\label{fig_adv-optalpha_quadfit_Ms240}\includegraphics[width = 0.4\textwidth]{figures/adv-optalpha-quadfit_Ms240.pdf}}
     \subfloat[]
       {\label{fig_adv-optalpha-asqrtx_fit_den_Mss.pdf}\includegraphics[width = 0.4\textwidth]{figures/adv-optalpha-asqrtx_fit_den_Mss.pdf}}
 \end{center}
 \caption{Optimal weight for adversarial networks. (a) Accuracy vs $\alpha$. (b) $\alpha_{opt}$ scales as $O(\sqrt{\Mt})$.}
 \label{fig_adv_results_alpha}
\end{figure}

\clearpage

\section{Conclusion}
\label{sec:conclusion}

We have presented a theoretical analysis of semi-supervised domain adaptation methods that jointly learn feature transformations that map the source and target domains to a shared space, along with a classifier defined in that space. We have first derived general performance bounds applicable to arbitrary function classes and domain discrepancy measures. We have then specialized these results under the assumption that the domain alignment is measured using the maximum mean discrepancy (MMD) metric. Our results show that the number of labeled source samples must scale logarithmically with the covering number of the combined hypothesis class comprising the feature transformation and the classifier, while the total sample sizes must scale logarithmically with the covering numbers of the feature transformation classes alone.

Building on these results, we have then extended our analysis to characterize the sample complexity of domain-adaptive neural networks. Our treatment relies on a detailed examination of the covering numbers of the corresponding function classes in deep architectures. We have focused on two types of neural networks, which perform domain alignment via MMD-based transformations or through adversarial objectives. In both cases, our analysis indicates that the sample complexities for both labeled and unlabeled data grow quadratically with the network depth and width. We have also shown that the scarcity of labeled target data can be effectively mitigated by scaling the weight of the target classification loss proportionally to the square root of the number of labeled target samples. 

To the best of our knowledge, our study provides the first comprehensive theoretical characterization of the sample complexity of domain-adaptive neural networks. 

\textbf{Relation to prior work.}\footnote{A detailed discussion is provided in the supplement.} Previous theoretical analyses of domain adaptation have primarily focused on how domain discrepancy affects generalization when learning a classifier in the original source and target domains, without considering domain-aligning transformations \cite{BenDavidBCP06,BenDavidBCKPV10,MansourMR09}. These works establish generalization bounds in terms of VC-dimensions or Rademacher complexities of hypothesis classes, combined with various distribution divergence measures. While theoretically insightful, many of these divergence measures are difficult to estimate in practice. In contrast, our results in Theorems \ref{thm:main_result_mmd}-\ref{thm_main_result_dann} provide practical generalization bounds based on empirical losses and distribution distances computed directly on aligned training data. Research on neural network sample complexity in single-domain settings \cite{NeyshaburTS15,WeiM19} has shown dependencies on network size; our work extends these insights to the domain adaptation setting, demonstrating quadratic scaling with both network depth and width. 


\section*{Acknowledgement}

The authors would like to thank \"Ozlem Akg\"ul, \"Omer Faruk Arslan, Atilla Can Aydemir, Firdevs Su Ayd{\i}n and Enes Ata \"Unsal for their help with the experiments in \cref{ssec_exp_mmd_dan}. 

\clearpage

\bibliographystyle{siamplain}
\bibliography{refs}

\end{document}
